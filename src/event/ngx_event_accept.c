
/*
 * Copyright (C) Igor Sysoev
 * Copyright (C) Nginx, Inc.
 */


#include <ngx_config.h>
#include <ngx_core.h>
#include <ngx_event.h>


static ngx_int_t ngx_disable_accept_events(ngx_cycle_t *cycle, ngx_uint_t all);
#if (NGX_HAVE_EPOLLEXCLUSIVE)
static void ngx_reorder_accept_events(ngx_listening_t *ls);
#endif
static void ngx_close_accepted_connection(ngx_connection_t *c);


/**
 * 
 * 参考ngx_event_process_init方法。为ngx_listening_t套接字连接ngx_connection_t的读事件的处理函数
 * 
 * 调用accept建立新连接
 * 处理新连接事件的回调函数
 */
void
ngx_event_accept(ngx_event_t *ev)
{
    socklen_t          socklen;
    ngx_err_t          err;
    ngx_log_t         *log;
    ngx_uint_t         level;
    ngx_socket_t       s;
    ngx_event_t       *rev, *wev;
    ngx_sockaddr_t     sa;
    ngx_listening_t   *ls;
    ngx_connection_t  *c, *lc;
    ngx_event_conf_t  *ecf;
#if (NGX_HAVE_ACCEPT4)
    static ngx_uint_t  use_accept4 = 1;
#endif

    //如果已经超时
    if (ev->timedout) {
        if (ngx_enable_accept_events((ngx_cycle_t *) ngx_cycle) != NGX_OK) {
            return;
        }

        ev->timedout = 0;
    }

     /* 获取ngx_event_core_module模块的配置项参数结构 */
    ecf = ngx_event_get_conf(ngx_cycle->conf_ctx, ngx_event_core_module);

    if (!(ngx_event_flags & NGX_USE_KQUEUE_EVENT)) {
        ev->available = ecf->multi_accept;
    }

    lc = ev->data;              /* 获取事件所对应的连接对象 */
    ls = lc->listening;         /* 获取连接对象的监听端口数组 */
    ev->ready = 0;              /* 设置事件的状态为未准备就绪 */

    ngx_log_debug2(NGX_LOG_DEBUG_EVENT, ev->log, 0,
                   "accept on %V, ready: %d", &ls->addr_text, ev->available);

    do {
        socklen = sizeof(ngx_sockaddr_t);

//调用accept方法试图建立新连接
#if (NGX_HAVE_ACCEPT4)
        if (use_accept4) {
            s = accept4(lc->fd, &sa.sockaddr, &socklen, SOCK_NONBLOCK);
        } else {
            s = accept(lc->fd, &sa.sockaddr, &socklen);
        }
#else
        //调用accept函数，从已连接队列得到一个连接以及对应的套接字
        s = accept(lc->fd, &sa.sockaddr, &socklen);
#endif

        /* 连接建立错误时的相应处理 */
        //如果没有准备好的新连接事件， ngx_event_accept方法会直接返回
        if (s == (ngx_socket_t) -1) {
            err = ngx_socket_errno;

            if (err == NGX_EAGAIN) {
                ngx_log_debug0(NGX_LOG_DEBUG_EVENT, ev->log, err,
                               "accept() not ready");
                return;
            }

            level = NGX_LOG_ALERT;

            if (err == NGX_ECONNABORTED) {
                level = NGX_LOG_ERR;

            } else if (err == NGX_EMFILE || err == NGX_ENFILE) {
                level = NGX_LOG_CRIT;
            }

#if (NGX_HAVE_ACCEPT4)
            ngx_log_error(level, ev->log, err,
                          use_accept4 ? "accept4() failed" : "accept() failed");

            if (use_accept4 && err == NGX_ENOSYS) {
                use_accept4 = 0;
                ngx_inherited_nonblocking = 0;
                continue;
            }
#else
            ngx_log_error(level, ev->log, err, "accept() failed");
#endif

            if (err == NGX_ECONNABORTED) {
                if (ngx_event_flags & NGX_USE_KQUEUE_EVENT) {
                    ev->available--;
                }

                if (ev->available) {
                    continue;
                }
            }

            if (err == NGX_EMFILE || err == NGX_ENFILE) {
                if (ngx_disable_accept_events((ngx_cycle_t *) ngx_cycle, 1)
                    != NGX_OK)
                {
                    return;
                }

                if (ngx_use_accept_mutex) {
                    if (ngx_accept_mutex_held) {
                        ngx_shmtx_unlock(&ngx_accept_mutex);
                        ngx_accept_mutex_held = 0;
                    }

                    ngx_accept_disabled = 1;

                } else {
                    ngx_add_timer(ev, ecf->accept_mutex_delay);
                }
            }

            return;
        }

#if (NGX_STAT_STUB)
        (void) ngx_atomic_fetch_add(ngx_stat_accepted, 1);
#endif

        /*
         * ngx_accept_disabled 变量是负载均衡阈值，表示进程是否超载；

         * 负载均衡阈值为每个进程最大连接数的八分之一减去空闲连接数；
         * 
         * ngx_accept_disabled 大于0（即当每个进程accept到的活动连接数超过最大连接数的7/8时），
         * 表示该进程处于负载过重， 就不再处理新的连接accept事件；
         * 
         */
        ngx_accept_disabled = ngx_cycle->connection_n / 8
                              - ngx_cycle->free_connection_n;

        //从连接池中获取一个ngx_connection_t连接对象
        c = ngx_get_connection(s, ev->log);     //获取一个connection结构体

        if (c == NULL) {
            if (ngx_close_socket(s) == -1) {
                ngx_log_error(NGX_LOG_ALERT, ev->log, ngx_socket_errno,
                              ngx_close_socket_n " failed");
            }

            return;
        }

        c->type = SOCK_STREAM;

#if (NGX_STAT_STUB)
        (void) ngx_atomic_fetch_add(ngx_stat_active, 1);
#endif

        //为新的连接创建一个连接池pool， 在这个连接释放到空闲连接池时，释放 pool内存池, 初始大小默认为256字节，可通过connection_pool_size指令设置
        c->pool = ngx_create_pool(ls->pool_size, ev->log);
        if (c->pool == NULL) {
            ngx_close_accepted_connection(c);
            return;
        }

        /*分配一个套接口地址（sockaddr），并将accept得到的对端地址拷贝在其中，保存在sockaddr字段；*/
        if (socklen > (socklen_t) sizeof(ngx_sockaddr_t)) {
            socklen = sizeof(ngx_sockaddr_t);
        }

        c->sockaddr = ngx_palloc(c->pool, socklen);
        if (c->sockaddr == NULL) {
            ngx_close_accepted_connection(c);
            return;
        }

        //设置socket地址
        ngx_memcpy(c->sockaddr, &sa, socklen);

        /* 分配日志结构，并保存在其中，以便后续的日志系统使用；*/
        log = ngx_palloc(c->pool, sizeof(ngx_log_t));
        if (log == NULL) {
            ngx_close_accepted_connection(c);
            return;
        }

        /* set a blocking mode for iocp and non-blocking mode for others */

        /* 设置套接字的属性 */
        if (ngx_inherited_nonblocking) {
            if (ngx_event_flags & NGX_USE_IOCP_EVENT) {
                //设为非阻塞套接字
                if (ngx_blocking(s) == -1) {
                    ngx_log_error(NGX_LOG_ALERT, ev->log, ngx_socket_errno,
                                  ngx_blocking_n " failed");
                    ngx_close_accepted_connection(c);
                    return;
                }
            }

        } else {
            if (!(ngx_event_flags & NGX_USE_IOCP_EVENT)) {
                if (ngx_nonblocking(s) == -1) {
                    ngx_log_error(NGX_LOG_ALERT, ev->log, ngx_socket_errno,
                                  ngx_nonblocking_n " failed");
                    ngx_close_accepted_connection(c);
                    return;
                }
            }
        }

#if (NGX_HAVE_KEEPALIVE_TUNABLE && NGX_DARWIN)

        /* Darwin doesn't inherit TCP_KEEPALIVE from a listening socket */

        if (ls->keepidle) {
            if (setsockopt(s, IPPROTO_TCP, TCP_KEEPALIVE,
                           (const void *) &ls->keepidle, sizeof(int))
                == -1)
            {
                ngx_log_error(NGX_LOG_ALERT, ev->log, ngx_socket_errno,
                              "setsockopt(TCP_KEEPALIVE, %d) failed, ignored",
                              ls->keepidle);
            }
        }

#endif

        *log = ls->log;

        /* 初始化连接相应的io收发函数，具体的io收发函数和使用的事件模型及操作系统相关 */
        c->recv = ngx_recv;
        c->send = ngx_send;
        c->recv_chain = ngx_recv_chain;
        c->send_chain = ngx_send_chain;

        c->log = log;
        c->pool->log = log;

        c->socklen = socklen;
        c->listening = ls;
        /**将本地套接口地址保存在local_sockaddr字段，因为这个值是从监听结构ngx_listening_t中可得，
         * 而监听结构中保存的只是配置文件中设置的监听地址，但是配置的监听地址可能是通配符*，即监听在所有的地址上，
         * 所以连接中保存的这个值最终可能还会变动，会被确定为真正的接收地址；
         * */
        c->local_sockaddr = ls->sockaddr;
        c->local_socklen = ls->socklen;

#if (NGX_HAVE_UNIX_DOMAIN)
        if (c->sockaddr->sa_family == AF_UNIX) {
            c->tcp_nopush = NGX_TCP_NOPUSH_DISABLED;
            c->tcp_nodelay = NGX_TCP_NODELAY_DISABLED;
#if (NGX_SOLARIS)
            /* Solaris's sendfilev() supports AF_NCA, AF_INET, and AF_INET6 */
            c->sendfile = 0;
#endif
        }
#endif

        /* 获取新连接的读事件、写事件 */
        rev = c->read;
        wev = c->write;

        /* 写事件准备就绪,nginx默认连接第一次为可写 */
        wev->ready = 1;

        if (ngx_event_flags & NGX_USE_IOCP_EVENT) {
            rev->ready = 1;
        }

        /* 如果监听套接字设置了 TCP_DEFER_ACCEPT 属性，则表示该连接上已经有数据包过来，于是设置读事件为就绪；*/
        if (ev->deferred_accept) {
            rev->ready = 1;
#if (NGX_HAVE_KQUEUE || NGX_HAVE_EPOLLRDHUP)
            rev->available = 1;
#endif
        }

        rev->log = log;
        wev->log = log;

        /*
         * TODO: MT: - ngx_atomic_fetch_add()
         *             or protection by critical section or light mutex
         *
         * TODO: MP: - allocated in a shared memory
         *           - ngx_atomic_fetch_add()
         *             or protection by critical section or light mutex
         */

        c->number = ngx_atomic_fetch_add(ngx_connection_counter, 1);

        //记录连接建立时间
        c->start_time = ngx_current_msec;

#if (NGX_STAT_STUB)
        (void) ngx_atomic_fetch_add(ngx_stat_handled, 1);
#endif

        /* 将sockaddr字段保存的对端地址格式化为可读字符串，并保存在addr_text字段； */
        if (ls->addr_ntop) {
            c->addr_text.data = ngx_pnalloc(c->pool, ls->addr_text_max_len);
            if (c->addr_text.data == NULL) {
                ngx_close_accepted_connection(c);
                return;
            }

            c->addr_text.len = ngx_sock_ntop(c->sockaddr, c->socklen,
                                             c->addr_text.data,
                                             ls->addr_text_max_len, 0);
            if (c->addr_text.len == 0) {
                ngx_close_accepted_connection(c);
                return;
            }
        }

#if (NGX_DEBUG)
        {
        ngx_str_t  addr;
        u_char     text[NGX_SOCKADDR_STRLEN];

        ngx_debug_accepted_connection(ecf, c);

        if (log->log_level & NGX_LOG_DEBUG_EVENT) {
            addr.data = text;
            addr.len = ngx_sock_ntop(c->sockaddr, c->socklen, text,
                                     NGX_SOCKADDR_STRLEN, 1);

            ngx_log_debug3(NGX_LOG_DEBUG_EVENT, log, 0,
                           "*%uA accept: %V fd:%d", c->number, &addr, s);
        }

        }
#endif

        /* 将新连接对应的读事件注册到事件监控机制中；
         * 注意：若是epoll事件机制，这里是不会执行，
         * 因为epoll事件机制会在调用新连接处理函数ls->handler(c)
         *（实际调用ngx_http_init_connection）时，才会把新连接对应的读事件注册到epoll事件机制中；
         */
        //将这个新连接对应的读事件添加到epoll等事件驱动模块中，这样，在这个连接上如果接收到用户请求epoll_wait，就会收集到这个事件
        if (ngx_add_conn && (ngx_event_flags & NGX_USE_EPOLL_EVENT) == 0) {
            if (ngx_add_conn(c) == NGX_ERROR) {
                ngx_close_accepted_connection(c);
                return;
            }
        }

        log->data = NULL;
        log->handler = NULL;

        //调用监听对象ngx_listening_t中的handler回调方法（在 ngx_http_add_listening 方法中设置的）。
        //ngx_listening_t结构体的handler回调方法就是当新的TCP连接刚刚建立完成时在这里调用的
        ls->handler(c); //为 ngx_http_init_connection, 初始化该连接结构的其他部分

        if (ngx_event_flags & NGX_USE_KQUEUE_EVENT) {
            ev->available--;
        }

    //当available为1时，告诉Nginx一 次性尽量多地建立新连接
    } while (ev->available);        //available 表示multi_accept

#if (NGX_HAVE_EPOLLEXCLUSIVE)
    ngx_reorder_accept_events(ls);
#endif
}


/**
 * 解决惊群问题
 *
 * 在打开accept_mutex锁的情况下，只有调用 ngx_trylock_accept_mutex 方法后，当前的worker进程才会去试着监听web端口
 * 
 * 在调用此方法后，要么是唯一获取到ngx_accept_mutex锁且其epoll等事件驱动模块开始监控Web端口上的新连接事件，
 * 要么是没有获取到锁，当前进程 不会收到新连接事件
 * 
 * 此函数执行结构是 ngx_accept_mutex_held， 为1表示已经获取到了锁，为0表示没有获取到锁。
 * 
 */
ngx_int_t
ngx_trylock_accept_mutex(ngx_cycle_t *cycle)
{
    //使用进程间的同步锁，试图获取 accept_mutex锁。注意，ngx_shmtx_trylock返回1表示成功拿到锁，返回0表示获取锁失败。
    //这个获取锁的过程是非阻塞的，此时一旦锁被其他 worker子进程占用， ngx_shmtx_trylock方法会立刻返回
    if (ngx_shmtx_trylock(&ngx_accept_mutex)) {

        ngx_log_debug0(NGX_LOG_DEBUG_EVENT, cycle->log, 0,
                       "accept mutex locked");

        //如果获取到 accept_mutex锁，但ngx_accept_mutex_held为 1，则立刻返回。 
        //ngx_accept_mutex_held是一个标志位，当它为 1时，表示当前进程已经获取到锁了
        if (ngx_accept_mutex_held && ngx_accept_events == 0) {
            return NGX_OK;
        }

        // 将所有监听连接的读事件（即accept事件）添加到当前的 epoll等事件驱动模块中
        if (ngx_enable_accept_events(cycle) == NGX_ERROR) {
            ///将监听句柄添加到事件驱动模块失败，就必须释放 ngx_accept_mutex锁
            ngx_shmtx_unlock(&ngx_accept_mutex);
            return NGX_ERROR;
        }

        //经过 ngx_enable_accept_events方法的调用，当前进程的事件驱动模块已经开始监听所有的端口，
        //这时需要把 ngx_accept_mutex_held标志位置为 1，方便本进程的其他模块了解它目前已经获取到了锁
        ngx_accept_events = 0;
        ngx_accept_mutex_held = 1;

        return NGX_OK;
    }

    ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle->log, 0,
                   "accept mutex lock failed: %ui", ngx_accept_mutex_held);

    //如果 ngx_shmtx_trylock返回 0，则表明获取 ngx_accept_mutex锁失败，这时如果 ngx_accept_mutex_held标志位还为 1，
    //即当前进程还在获取到锁的状态，这当然是不正确的，需要处理
    if (ngx_accept_mutex_held) {
        //ngx_disable_accept_events会将所有监听连接的读事件从事件驱动模块中移除
        if (ngx_disable_accept_events(cycle, 0) == NGX_ERROR) {
            return NGX_ERROR;
        }

        //在没有获取到 ngx_accept_mutex锁时，必须把 ngx_accept_mutex_held置为0
        ngx_accept_mutex_held = 0;
    }

    return NGX_OK;
}


/**
 * 获取到负载均衡锁后调用，开启所有监听套接字的accept事件epoll监听
 * 
 * 将所有监听连接的读事件（即accept事件）添加到当前的 epoll等事件驱动模块中
 * 
 * */ 
ngx_int_t
ngx_enable_accept_events(ngx_cycle_t *cycle)
{
    ngx_uint_t         i;
    ngx_listening_t   *ls;
    ngx_connection_t  *c;

    /* 获取监听数组的首地址 */
    ls = cycle->listening.elts;
    /* 遍历整个监听数组 */
    for (i = 0; i < cycle->listening.nelts; i++) {

        /* 获取当前监听socket所对应的连接 */
        c = ls[i].connection;

        /* 当前连接的读事件是否处于active活跃状态 */
        if (c == NULL || c->read->active) {
            continue;
        }

        /* 若当前连接的读事件不在事件监控对象中，则将其加入 */
        if (ngx_add_event(c->read, NGX_READ_EVENT, 0) == NGX_ERROR) {
            return NGX_ERROR;
        }
    }

    return NGX_OK;
}


/**
 * 未获取到负载均衡锁后调用，
 * 
 * 将所有监听连接的读事件(即accept事件)从 epoll等事件驱动模块中移除
 * 
 * */ 
static ngx_int_t
ngx_disable_accept_events(ngx_cycle_t *cycle, ngx_uint_t all)
{
    ngx_uint_t         i;
    ngx_listening_t   *ls;
    ngx_connection_t  *c;

    /* 遍历所有监听接口 */
    ls = cycle->listening.elts;
    for (i = 0; i < cycle->listening.nelts; i++) {

          /* 获取监听接口对应的连接 */
        c = ls[i].connection;

        if (c == NULL || !c->read->active) {
            continue;
        }

#if (NGX_HAVE_REUSEPORT)

        /*
         * do not disable accept on worker's own sockets
         * when disabling accept events due to accept mutex
         */

        if (ls[i].reuseport && !all) {
            continue;
        }

#endif

        /* 从事件驱动模块中移除连接 */
        if (ngx_del_event(c->read, NGX_READ_EVENT, NGX_DISABLE_EVENT)
            == NGX_ERROR)
        {
            return NGX_ERROR;
        }
    }

    return NGX_OK;
}


#if (NGX_HAVE_EPOLLEXCLUSIVE)

static void
ngx_reorder_accept_events(ngx_listening_t *ls)
{
    ngx_connection_t  *c;

    /*
     * Linux with EPOLLEXCLUSIVE usually notifies only the process which
     * was first to add the listening socket to the epoll instance.  As
     * a result most of the connections are handled by the first worker
     * process.  To fix this, we re-add the socket periodically, so other
     * workers will get a chance to accept connections.
     */

    if (!ngx_use_exclusive_accept) {
        return;
    }

#if (NGX_HAVE_REUSEPORT)

    if (ls->reuseport) {
        return;
    }

#endif

    c = ls->connection;

    if (c->requests++ % 16 != 0
        && ngx_accept_disabled <= 0)
    {
        return;
    }

    if (ngx_del_event(c->read, NGX_READ_EVENT, NGX_DISABLE_EVENT)
        == NGX_ERROR)
    {
        return;
    }

    if (ngx_add_event(c->read, NGX_READ_EVENT, NGX_EXCLUSIVE_EVENT)
        == NGX_ERROR)
    {
        return;
    }
}

#endif


static void
ngx_close_accepted_connection(ngx_connection_t *c)
{
    ngx_socket_t  fd;

    ngx_free_connection(c);

    fd = c->fd;
    c->fd = (ngx_socket_t) -1;

    if (ngx_close_socket(fd) == -1) {
        ngx_log_error(NGX_LOG_ALERT, c->log, ngx_socket_errno,
                      ngx_close_socket_n " failed");
    }

    if (c->pool) {
        ngx_destroy_pool(c->pool);
    }

#if (NGX_STAT_STUB)
    (void) ngx_atomic_fetch_add(ngx_stat_active, -1);
#endif
}


u_char *
ngx_accept_log_error(ngx_log_t *log, u_char *buf, size_t len)
{
    return ngx_snprintf(buf, len, " while accepting new connection on %V",
                        log->data);
}


#if (NGX_DEBUG)

void
ngx_debug_accepted_connection(ngx_event_conf_t *ecf, ngx_connection_t *c)
{
    struct sockaddr_in   *sin;
    ngx_cidr_t           *cidr;
    ngx_uint_t            i;
#if (NGX_HAVE_INET6)
    struct sockaddr_in6  *sin6;
    ngx_uint_t            n;
#endif

    cidr = ecf->debug_connection.elts;
    for (i = 0; i < ecf->debug_connection.nelts; i++) {
        if (cidr[i].family != (ngx_uint_t) c->sockaddr->sa_family) {
            goto next;
        }

        switch (cidr[i].family) {

#if (NGX_HAVE_INET6)
        case AF_INET6:
            sin6 = (struct sockaddr_in6 *) c->sockaddr;
            for (n = 0; n < 16; n++) {
                if ((sin6->sin6_addr.s6_addr[n]
                    & cidr[i].u.in6.mask.s6_addr[n])
                    != cidr[i].u.in6.addr.s6_addr[n])
                {
                    goto next;
                }
            }
            break;
#endif

#if (NGX_HAVE_UNIX_DOMAIN)
        case AF_UNIX:
            break;
#endif

        default: /* AF_INET */
            sin = (struct sockaddr_in *) c->sockaddr;
            if ((sin->sin_addr.s_addr & cidr[i].u.in.mask)
                != cidr[i].u.in.addr)
            {
                goto next;
            }
            break;
        }

        c->log->log_level = NGX_LOG_DEBUG_CONNECTION|NGX_LOG_DEBUG_ALL;
        break;

    next:
        continue;
    }
}

#endif
